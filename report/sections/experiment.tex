\section{Experiment}
\label{sec:experiment}

When ran with all the options activated, our tool first collects the data from 
a \verb|json| file containing our dataset, then loads the words occurrences 
from a file which has been previously generated and computes the term frequency 
as well as the term frequency invert document frequency.

Once done, the features are extracted and the instances and labels are built as 
lists. The entropy is computed over the labels.

From that point, classification, cross-validation and algorithm tournament are 
computed.

\subsection{Classification}

To perform the classification the train set is set as $75\%$ of the 
instances, the test set being the $25\%$ remaining.
In the same way, the train labels are set as $75\%$ of the labels and the test 
labels are the $25\%$ remaining.

Then, the classification routine is run. It iterates over all the 
selected classifiers and computes their accuracy and predictions.

The table \ref{tab:accuracy} shows the accuracy obtained during the 
classification process. The best ones are the \emph{Maximum Entropy},
\emph{LDA} and \emph{Naive Bayes}.


The accuracy of the Support Vector Machine is very bad, however this is not 
surprising because we did not reduce the dimensionality.

\begin{table}[!h]
 \centering
 \begin{tabular}{|l|c|}
  \hline
  \tabhead{Classifier} &
  \multicolumn{1}{|p{0.4\columnwidth}|}{\centering\tabhead{Accuracy}} \\
  \hline
  MaxEnt & 91.99\%\\
  LDA & 91.84\%\\
  Naive Bayes & 91.77\%\\
  Decision Tree & 86.97\%\\
  SVM (with RBF Kernel) & 60.05\%\\
  SVM (with Sigmoid Kernel) & 59.78\%\\
  Majority Vote & 59.78\%\\
  \hline
 \end{tabular}
 \caption{Accuracy of the classifications}
 \label{tab:accuracy}
\end{table}

\subsection{Cross-validation}

Once the classification is done, the cross-validation is computed. It allows us 
to validate our classification results.

For the task, the data is partitioned into complementary subsets.
The analysis is performed on the training subset while it is validated by the 
testing subset. Ten rounds are performed using a partition of $10\%$ - $90\%$.
The results of the cross-validation are then being used to by the 
algorithm tournament.

The average of the results from the cross-validation for each classifier is 
shown in table \ref{tab:cross}.

\begin{table}[!h]
 \centering
 \begin{tabular}{|l|c|}
  \hline
  \tabhead{Classifier} &
  \multicolumn{1}{|p{0.4\columnwidth}|}{\centering\tabhead{Average accuracy}} \\
  \hline
  MaxEnt & 92.10\%\\
  LDA & 92.04\%\\
  Naive Bayes & 92.00\%\\
  Decision Tree & 87.35\%\\
  Majority Vote & 60.91\%\\
  SVM (with RBF Kernel) & N/A\\
  SVM (with Sigmoid Kernel) & N/A\\
  \hline
 \end{tabular}
 \caption{Average accuracy gotten from the cross-validation}
 \label{tab:cross}
\end{table}

By comparing these results with the one from the classification, we can notice 
that they are very close to each another. The small difference is explained by 
the different partitioning of the dataset as the train set was less than half 
of the train set of the classification. Thus, the results of the 
cross-validation validates the results obtained in the classification step.

There is no results for the two SVMs classifiers as their computation is very 
long and it would need to be computed ten times. However, we can expect similar 
results.

\subsection{Algorithm tournament}

The algorithm tournament compares the results of the different classifiers, 
each one against each other.
Due to the lack of having a powerful enough computer, we were not able to use 
the support vector machine classifiers in the tournament as the computation 
times is quite long with such a dataset and having so many features to use.

The table \ref{tab:mcnemar} shows the results of each round of the tournament. 
It gives the winner, the \emph{Chi-Square} and the \emph{p-value}. We defined 
the \emph{Chi-Square} as follow :

\begin{equation} \label{eq:chi-squared2}
  \chi^2 = {\frac{(|b-c|-1)^2}{b+c}}
\end{equation}

The McNemar test with the bonferroni adjustment says that the comparison is
significant if :

\begin{equation} \label{eq:bonferroni}
	p-value < {\frac{\alpha}{n}}
\end{equation}

Where $\alpha = 0.05$ and $n$ is the number of duel done during the tournament. 
In our case, $n = 10$, so the \emph{p-value} must be smaller than $0.005$. As 
shown in the table \ref{tab:mcnemar}, all of our \emph{p-values} are smaller 
than $0.005$, therefore, these results are statistically significant.

\begin{table}[!h]
 \centering
 \begin{tabular}{|l|c|c|c|}
  \hline
  \tabhead{Classifiers} &
  \multicolumn{1}{|p{0.2\columnwidth}|}{\centering\tabhead{Winner}} &
  \multicolumn{1}{|p{0.2\columnwidth}|}{\centering\tabhead{Chi-Square}} &
  \multicolumn{1}{|p{0.2\columnwidth}|}{\centering\tabhead{p-value}} \\
  \hline
  NB vs DT & NB & 1210.79 & 0.0\\
  NB vs MaxEnt & MaxEnt & 16.32 & 5.33e-05\\
  NB vs MV & NB & 13149.10 & 0.0\\
  NB vs LDA & LDA & 18.05 & 2.15e-05\\
  MaxEnt vs DT & MaxEnt & 1330.43 & 0.0\\
  MaxEnt vs MV & MaxEnt & 12895.79 & 0.0\\
  MaxEnt vs LDA & MaxEnt & 10.06 & 0.0015\\
  DT vs MV & DT & 7440.81 & 0.0\\
  DT vs LDA & LDA & 1239.25 & 0.0\\
  MV vs LDA & LDA & 13174.91 & 0.0\\
  \hline
 \end{tabular}
 \caption{This table contains the results obtained during the algorithm 
	  tournament}
 \label{tab:mcnemar}
\end{table}

The table \ref{tab:ranking} shows the ranking of the tournament. The winner of 
the latter is the Maximum Entropy classifier while the loser one is the 
Majority Vote. The Decision Tree has a quite low score, which is normal due to 
the dimensionality of the problem.

\begin{table}[!h]
 \centering
 \begin{tabular}{|l|c|}
  \hline
  \tabhead{Classifier} &
  \multicolumn{1}{|p{0.7\columnwidth}|}{\centering\tabhead{Score}} \\
  \hline
  MaxEnt & 37.5\\
  LDA & 31.0\\
  Naive Bayes & 21.5\\
  Decision Tree & 10.0\\
  Majority Vote & 0.0\\
  \hline
 \end{tabular}
 \caption{This table shows the final ranking of the tournament.}
 \label{tab:ranking}
\end{table}